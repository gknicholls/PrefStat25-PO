#return(sum(dnorm(th,mean=0,sd=3,log=TRUE))) #covariates and response are O(1)
return(sum(dt(th,df=3,log=TRUE))) #covariates and response are O(1)
}
log.prior.sigma <- function(sigma) {
return(-log(sigma)) #Jeffreys
}
#prior over models - binomial(p,xi) (say, puts each covariate in model wp xi)
log.prior.model<-function(z,pr=xi,nc=p) {
ne=sum(z);
return(ne*log(xi)+(nc-ne)*log(1-xi))
}
library(MCMCpack)
#setup MCMC
K=10000000; SS=1000; Nsamp=K/SS; #run for lect was 100 times longer
TH=Z=THZ=matrix(NA,Nsamp,p);
colnames(TH)<-colnames(THZ)<-colnames(Z)<-colnames(X);
SG=matrix(NA,Nsamp,1);
colnames(SG)<-c('sigma')
#fixed dimension rw MH jump size
w=1;
#initialise at the MLE with all pars in the model
z=rep(1,p);                      #parameter i=1,...,p is in z[i]=1 or out z[i]=0
theta=coef(sw.lm)           #init all in and MLE
sigma=summary(sw.lm)$sigma  #init sigma using residual standard error
#run MCMC
for (k in 1:K) {
#vary the z's
i=sample(1:p,1);    #choose a parameter to toggle in/out
zp=z; zp[i]=1-z[i]; #propose the new model
#if (all(diff(zp)!=1)) {
MHR_num=log.lkd(theta,zp,sigma,y)+log.prior.model(zp);
MHR_den=log.lkd(theta,z,sigma,y)+log.prior.model(z);
if (log(runif(1))<(MHR_num-MHR_den)) {
z=zp
}
#}
#vary theta
i=sample(1:p,1);
thetap=theta; thetap[i]=theta[i]+w*(2*runif(1)-1) #propose new theta
MHR_num=log.prior.theta(thetap)+log.lkd(thetap,z,sigma,y);
MHR_den=log.prior.theta(theta)+log.lkd(theta,z,sigma,y);
if (log(runif(1))<(MHR_num-MHR_den)) {
theta=thetap
}
u=runif(1,min=0.5,max=2); sigmap=sigma*u; #propose new sigma by scaling
MHR_num=log.lkd(theta,z,sigmap,y)+log.prior.sigma(sigmap);
MHR_den=log.lkd(theta,z,sigma,y)+log.prior.sigma(sigma);
if (log(runif(1))<(MHR_num-MHR_den-log(u))) {
sigma=sigmap
}
if (k%%SS==0) {
TH[k/SS,]=theta;
Z[k/SS,]=z;
THZ[k/SS,]=theta*z;
SG[k/SS]=sigma;
}
}
#superficial convergence check based on inspection of MCMC traces and ESS
Nsamp=dim(Z)[1]
s=seq(10,Nsamp,by=5)
#dev.off()
(es.ss<-effectiveSize(as.mcmc(cbind(THZ,SG))))
#posterior probabilities for the different models
(Zpt.ss<-sort(round(100*table(Zs<-apply(Z,1,paste,collapse=''))/Nsamp,1),decreasing=TRUE))
#marginal posterior probability for each effect
#apply(Z,2,mean)
dZ=apply(Z,1,sum); (dZt.ss=100*table(dZ)/Nsamp)
hz=hist(dz,xlab='dim=sum_i z_i',main='dim distribution S&S')
#Bayes Methods - Normal-Dirichlet mixture
#fit to galaxy data
library(MCMCpack) #dinvgamma for transparency
###############################################
#MCMC fitting the galaxy data
y=scan("http://www.stats.ox.ac.uk/~nicholls/BayesMethods/galaxies.txt") #82 galaxy transverse velocities
nd=length(y)           #number of data points
#the priors for the means are not quite the same as those I use in the RJ-MCMC example
#because I want the priors to be conjugate I use inverse gamma for sigma^2 rather than
#gamma on sigma as I used before mu~N(20,10) as before, but sigma^2~IGamma(alpha0,beta0)
#the choice of alpha0 and beta0 is intended to give a similar prior for sigma
log.prior.param<-function(mu,sg,mu0=20,sigma0=10,alpha0=2,beta0=1/9) {
a=sum(dnorm(mu,mean=mu0,sd=sigma0,log=TRUE))
b=sum(log(dinvgamma(sg^2,shape=alpha0,scale=beta0)))
return(a+b)
}
sigma0=10; mu0=20
alpha0=2; beta0=1/9 #IGamma sg^2 mean=beta/(alpha-1)=9 so sg~3
#choice of alpha based on simulations above
alpha=1
#initialise MCMC state - S is known given mu but it is convenient to maintain both
S=S.init=rep(1,nd)
mu=mu.init=mean(y)
sg=sg.init=sd(y)
#run parameters and output storage
J=10000; SS=10;  #pretty minimal but enough for plausible estimates
CL=matrix(NA,J/SS,nd); PL=matrix(NA,J/SS,3); TH=list();
#debug check
db=1 #set to 0 to get CRP prior over dbns
#MCMC J steps each step sweeps mu, sigma and the cluster membership
for (j in 1:J) {
nS=table(S) #the number of pts in each cluster
K=max(S)    #the number of clusters
#go through each cluster and update mu[k] and sg[k] given the cluster membership S
for (k in 1:K) {
i.in.k=which(S==k); yk=y[i.in.k] #which data points in this cluster
nk=nS[k]                         #how many pts in this cluster
if (nk>0) {
#sample mu[k] given mu[-k], sg and S
sgk=sg[k]
bm=sqrt(1/(nk/sgk^2+1/sigma0^2))
am=bm^2*(sum(yk)/sgk^2+mu0/sigma0^2)
mu[k]=rnorm(1,mean=am,sd=bm)
#sample sg[k] given sg[-k], mu and S
as=alpha0+nk/2
bs=beta0+(1/2)*sum((yk-mu[k])^2)
sg[k]=1/sqrt(rgamma(1,shape=as,rate=bs))
}
}
#go through each point y[i] and update its cluster membership S[i]
for (i in 1:nd) {
#the number of pts per cluster and the number of clusters may have changed
nS=table(S)
K=max(S)
#we need the weights with i removed
nSp=nS;
k=S[i];
if (nS[k]==1) {
nSp[k]=alpha
pold=dnorm(y[i],mean=mu,sd=sg)^db*nSp
pnew=0
} else {
nSp[k]=nSp[k]-1
pold=dnorm(y[i],mean=mu,sd=sg)^db*nSp
#if we move i to a new cluster we need mu and sg values for the new cluster
#only possible if nS[i]>1
mkn=rnorm(1,mean=mu0,sd=sigma0)
sgn=1/sqrt(rgamma(1,shape=alpha0,rate=beta0))
#alpha*p(y[i]|mu.new,sig.new)
pnew=alpha*dnorm(y[i],mean=mkn,sd=sgn)^db
}
#the new cluster is either one of the old ones or a new one
#if nS[i]=1 then pr[1]=0 (cant move to a new K+1-cluster)
pr=c(pnew,pold);
#Normalise
pr=pr/sum(pr)
#pick a new cluster
kn=sample(0:K,1,prob=pr)
if (kn==0) { #new cluster, append mu.new and sig.new to mu, sg
S[i]=K+1
mu=c(mu,mkn)
sg=c(sg,sgn)
} else {     #put the pt in cluster kn
S[i]=kn
}
#keeping the cluster labels packed (ie 1:K) and sorted (ie min(S_k)<min(S_k') -> k<k')
#probably a bad choice of data structure it cant be this hard! Bugzone.
if (nS[k]==1 & kn!=k) {
#pack the labels
ib=which(S>k);
S[ib]=S[ib]-1;
mu=mu[-k];
sg=sg[-k]
#standardise labeling
b=rep(NA,max(S)); b[S[1]]=1
d=1; So=S;
for (a in 1:length(S)) {
x=S[a];
if (is.na(b[x])) {
d=d+1
b[x]=d;
}
So[a]=b[x]
}
o=order(b)
mu=mu[o]; sg=sg[o]
}
}
#collect samples from the MCMC every SS steps
if (j%%SS==0) {
#conditional on the number in each cluster we know the dbn of the cluster weights w
TH[[j/SS]]=list(w=rdirichlet(1,alpha+table(S)),mu=mu,sigma=sg)
CL[j/SS,]=S
PL[j/SS,]=c(length(mu),min(S),max(S))  #check we are keeping the cluster labels 'packed'
}
}
###
#Superficial Output Analysis
effectiveSize(as.mcmc(CL))
library(lattice);
plot(as.mcmc(PL[,1]))
###
###
#visualise co-occurence matrix p(i,j) = prob i,j in same cluster
Nsamp=J/SS;
com=matrix(0,nd,nd)
for (i in 1:nd) {
for (j in 1:nd) {
for (m in 1:Nsamp) {
com[i,j]=com[i,j]+(CL[m,i]==CL[m,j])
}
}
}
com=com/(J/SS)
image(com)
###
###
#posterior dbn over number of components - exactly the same as for RJ-MCMC
#pdf('DPmixtureGhistM.pdf',8,4)
hist(d<-PL[,1],breaks=0.5:(0.5+max(PL[,1])),main='',freq=FALSE,
xlab='Number of components m in mixture',ylab='density',xlim=c(0,1+max(PL[,1])))
#dev.off()
table(d)/Nsamp
#loglkd including the weights (not used in MCMC)
log.lkd<-function(y,w,mu,sigma) {
tot=0
for (i in 1:length(y)) {
tot=tot+log(sum(w*dnorm(y[i],mean=mu,sd=sigma)))
}
return(tot)
}
###
#compute posterior predictive distributions - weights (w) vs. means (mu) scater plot
#pdf('DPmixtureGscatterMUW.pdf',8,4)
L=200
x=seq(0,40,length.out=L)
den=matrix(0,max(d)-min(d)+1,L)
plot(c(),c(),xlim=c(0,50),ylim=c(0,1),xlab='MU, colored by number of clusters in state',ylab='W component weight')
for (k in 1:Nsamp) {
w=TH[[k]]$w; mu=TH[[k]]$mu; sigma=TH[[k]]$sigma
ind=d[k]-min(d)+1
points(mu,w,pch='.',col=ind)
den[ind,]=den[ind,]+exp(apply(t(x),2,log.lkd,w,mu,sigma))
}
cden=den/as.vector(table(d))
mden=apply(den,2,sum)/Nsamp
#dev.off()
#plot posterior predictive distributions
#pdf('DPmixtureGppd.pdf',8,4)
#data
hist(y,breaks=x,freq=FALSE,main='',xlab='velocity',ylab='density')
#posterior predicitve mean
for (i in 1:3) {#dim(cden)[1]) {
#posteior preditive dbn conditioned on "i" components
lines(x,cden[i,],lwd=2,col=i+1)
}
lines(x,mden,lwd=2,col=1)
#dev.off()
#shows the label switching if we dont sort mu at plot
#pdf('DPmixtureLS.pdf',8,4)
plot(c(1,Nsamp),c(0,0),type='n',ylim=c(0,50),ylab='mu',xlab='MCMC step (x 100)')
for (i in 1:Nsamp) {
points(rep(i,PL[i,1]),(TH[[i]]$mu),pch='.',cex=2,col=1:PL[i,1])
}
#dev.off()
gpars<-par()
i=match(c("cin","cra","csi","cxy","din","page"),names(gpars))
gpars<-gpars[-i]
library(MASS)
library(mnem)       # needed for transitive.reduction/closure
library(igraph)
library(Rgraphviz)
library(graph)
library(coda)       # for MCMC output analysis
library(lecount)    # count linear extensions of a PO
library(partitions) # used by dimension functions
#update this to suit yourself
wd<-"C:/Users/nicholls.NICHOLLS2389/OneDrive - Nexus365/Documents/GitHub/PrefStat25-PO/Code for Practical/"
setwd(wd)
source(file="pofun.R")
source(file="pofun_aux.R")
source(file="modelfun.R")
source(file="dimfun.R")
set.seed(10);
M=8
h<-rZPO(M)   # for now this is just a random PO
#h[i,j]=1 if i>j
h
sum(h)
choose(M,2)-sum(h)
showDAG(h,edge.color='black',vertex.color=NA,vertex.size=25,edge.arrow.size=0.5)
title('transitive closure of h')
# find the list of top nodes - no "in" edges
(top=which(apply(h,2,sum)==0))
# list the bottom nodes - no "out" edges
(bot=which(apply(h,1,sum)==0))
dagdepth(h)
h.empty=0*h
h.total=h.empty; h.total[upper.tri(h.total)]<-1
par(mfrow=c(1,2))
showDAG(h.empty,vertex.color=NA,vertex.size=25)
title('Empty order')
showDAG(h.total,edge.color='black',vertex.color=NA,vertex.size=15,edge.arrow.size=0.25)
title('Complete order')
par(gpars)
dagdepth(h.empty) #depth is the length of longest chain, counting vertices in chain
dagdepth(h.total); dagdepth(h.total)==M
hr<-transitive.reduction(h)
showDAG(hr,edge.color='black',vertex.color=NA,vertex.size=25,edge.arrow.size=0.5)
title('transitive reduction of h')
vcols=rep(NA,M);
vcols[top]<-'lightgreen'; vcols[bot]<-'pink'
# igraph orders the edges by first vertex
i=which(t(h)==1)
j=which(t(h)!=t(hr))
j.in.i=match(j,i)
ecols=rep('black',length(i))
ecols[j.in.i]<-'lightblue'
showDAG(h,vertex.color=vcols,edge.color=ecols,vertex.size=25,
edge.arrow.size=0.5)
legend('topright',cex=0.9,lty=c(1,1,NA,NA),pch=c(NA,NA,16,16),col=c('black','lightblue','lightgreen','pink'),lwd=c(2,2,NA,NA),
legend=c('reduction','closure','max set','min set'),bty="n")
v=matrix(c(1,2,3,1,3,2),3,2)
v
hi<-intersect.TO(v)
hi
showDAG(hi,edge.color='black',vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
par(mfrow=c(1,2))
M=4
b<-matrix(c(0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0),M,M,byrow=TRUE)
row.names(b)<-colnames(b)<-1:M
showDAG(b,edge.color='black',vertex.color=NA,vertex.size=45,edge.arrow.size=0.5)
title('example partial order')
(le<-le.expand(b));
dim(le)[2];
nle(b);
bc<-intersect.TO(le)
# poset bc should be the same as b
showDAG(bc,edge.color='black',vertex.color=NA,vertex.size=45,edge.arrow.size=0.5)
title('intersection of its LEs')
par(gpars)
le
(b.realiser<-decompose(b))
all(intersect.TO(b.realiser)==b) # intersect LEs in realiser gives back PO
dimension(b)
cpo=crown.PO(6)
showDAG(cpo,edge.color='black',vertex.color=NA,vertex.size=45,edge.arrow.size=0.5)
le<-le.expand(cpo)
le
(cpo.realiser<-decompose(cpo))     # takes a couple of seconds
all(intersect.TO(cpo.realiser)==cpo)
M=5
h=matrix(c(0,1,1,1,1,0,0,1,0,1,0,0,0,0,1,0,0,0,0,1,0,0,0,0,0),M,M,byrow=TRUE)
colnames(h)<-rownames(h)<-1:M
showDAG(transitive.reduction(h),edge.color='black',vertex.color=NA,vertex.size=25,edge.arrow.size=0.5)
(le=le.expand(h))
q=le[,1] # start state must be a LE
T=10000  # simulate T steps
X<-matrix(NA,M,T) # store the state of the queue
for (t in 1:T) {
i=sample(1:(M-1),1)         # propose to swap q[i] and q[i+1]
if (h[q[i],q[i+1]]!=1) {    # if q[i+1] isnt below q[i] in the PO
q[c(i,i+1)]=q[c(i+1,i)]   # swap q[i] and q[i+1]
}
X[,t]=q                     # save the current queue state
}
ord=apply(X,2,function(x){
which(apply(le,2,function(y){all(x==y)}))
})
(ft<-table(ord)/T)
le.str<-apply(le,2,function(x){paste(x,collapse = ' ')})
barplot(ft~le.str,las=2,cex.names=0.5,
ylab='probability',xlab='',
main='distribution of random orders - noise free case',
cex.main=0.8,cex.axis=0.7);
abline(h=1/nle(h),col=2,lwd=2,lty=2)
set.seed(10)
M=8
h.true<-rZPO(M)       # make a random PO with n items
showDAG(transitive.reduction(h.true),edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
title("true PO")
N=10                 # try experimenting with smaller N
Y<-matrix(NA,M,N)
for (i in 1:N) {
Y[,i]<-PunifLE(h.true)
}
Y
h.hat<-intersect.TO(Y)
par(mfrow=c(1,2))
showDAG(transitive.reduction(h.hat),edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
title("MLE PO")
showDAG(transitive.reduction(h.true),edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
title("true PO")
nle(h.true)
decompose(h.true) # smallest number of lists that identify the PO
set.seed(11)
N=10
size=sample(2:M,N,replace=TRUE)
S<-lapply(size,function(x){sample(1:M,x,replace=FALSE)})
h.true.sub<-lapply(S,function(x){suborder(h.true,x)})
showDAGs(B=1,T=N,PO=h.true.sub)
par(gpars)
Y<-vector('list',N)
for (i in 1:N) {
Y[[i]]<-PunifLE(h.true.sub[[i]])
}
Y
h.est<-intersect.SO(Y)
par(mfrow=c(1,2));
showDAG(transitive.reduction(h.true),edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
title("true PO")
showDAG(transitive.reduction(h.est),edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
title("estimated PO")
mv<-seq(8,16,2);   # if no lecount()
#mv<-seq(8,16,2);   # if no lecount()
mv<-seq(35,47,2); # if lecount() loaded - unwise to try above 50 (memory)
mvl=length(mv); rt<-numeric(mvl)
pb<-txtProgressBar(min=1, max=mvl, style=3)
for (i in 1:mvl) {
h<-crown.PO(mv[i])
rt[i]<-system.time({nle(h)})[3] # how long to calculate # LEs?
setTxtProgressBar(pb, i)
flush.console()
}
# The runtime measured for m=8 may be zero - ignore warning
par(gpars)
plot(mv,rt,log='y',type='l',ylab='run time',xlab='number of items');
points(mv,rt,pch=4)
set.seed(1)
M<-20
#  N=number of samples, S=subsample step, T=number of steps
N=100
S=M^2
T=N*S
POu<-rupo(M,N,S,T,DRAW=TRUE)
s=10; step=floor(N/s);
showDAGs(B=1,T=N,poi=seq(step,N,step),PO=POu)
Du=sapply(POu,dagdepth)
effectiveSize(Du) # if interested in MCMC mixing and convergence
par(gpars); plot(Du,type='l',ylab='Depth of random PO',xlab='MCMC step',
main='evolving depth of PO sampled U.A.R.')
hist(Du,breaks=seq(0.5,M+0.5,1),xlab='PO depth',
main='Depth dbn, uniform PO prior',freq=FALSE)
table(Du)/length(Du)
set.seed(6)
M=6 # number of items/nodes in PO
K=3 # number of features (columns of U/Z)
(rho=rRprior())  # the correlation within of rows of U
Sig=matrix(rho,K,K); diag(Sig)=1; # correlation matrix
U=mvrnorm(M,rep(0,K),Sig) # the feature matrix U, one row for each item
vp<-latent2order(U) # convert each column k+1:K to a total order, ranking by U[,k]
mu<-order2partial(vp,M); # intersect the column orders
muc<-my.transitive.closure(mu) # take the transitive closure
mur<-transitive.reduction(mu)  # and transitive reduction
par(mfrow=c(2,2),mai=c(0,0,0,0))
plot(0,0,xlim=c(1,K+1),ylim=c(-4,4),type='n',axes=FALSE,ann=FALSE)
cl=0; apply(U,1,function(x){lines(x,lwd=2,col=(cl<<-cl+1))})
title(expression('U'),line=-3)
legend('topright',legend=1:M,lty=1,col=1:M,lwd=2)
showDAG(mur,vertex.color=NA,vertex.size=25)
title(expression('h(U)'),line=-6)
alpha=rnorm(M)
Z=U+alpha # features with covariate additive effect offset
vp<-latent2order(Z)
mz<-order2partial(vp,M);
mzc<-my.transitive.closure(mz)
mzr<-transitive.reduction(mz)
plot(0,0,xlim=c(1,K+1),ylim=c(-4,4),type='n',axes=FALSE,ann=FALSE)
cl=0; apply(Z,1,function(x){lines(x,lwd=2,col=(cl<<-cl+1))})
title(expression(paste("Z=U+X",beta)),line=-3)
showDAG(mzr,vertex.color=NA,vertex.size=25)
title(expression('h(Z)'),line=-6)
title(expression('h(Z)'),line=-10)
par(gpars)
M=20
K=10
N=1000
POl<-vector('list',N)
pb<-txtProgressBar(min=0, max=N, style=3)
for (t in 1:N) {
beta=rnorm(M)
POl[[t]]<-my.transitive.closure(rZPO(M,K,b=beta))
setTxtProgressBar(pb, t/N); flush.console()
}
Dl=sapply(POl,dagdepth)
hist(Dl,breaks=seq(0.5,M+0.5,1),xlab='PO depth',
main='Depth dbn, latent feature PO prior',freq=FALSE)
table(Dl)/length(Dl)
plot(density(Du,bw=0.5,from=1,to=M),xlim=c(0,M+1),
main='prior depth dbns',xlab='depth')
lines(density(Dl,bw=0.5,from=1,to=M),col=2)
abline(v=c(1,M),col=3)
legend('topright',legend=c('Uniform','Latent'),col=c(1,2),lty=c(1,1),lwd=2)
M=4
h<-matrix(c(0,1,0,0,0,0,0,0,0,1,0,1,0,0,0,0),M,M,byrow=TRUE)
row.names(h)<-colnames(h)<-1:M
showDAG(h,edge.color='black',
vertex.color=NA,vertex.size=35,edge.arrow.size=0.5)
T=10000;
X=matrix(NA,M,T)
p.val=0.5 # the probability the next item in the list is chosen randomly
for (t in 1:T) {
X[,t]=PunifLE(h,p=p.val)
}
perm=t(enum.seq(1:M))
perm.str=apply(perm,2,function(x){paste(x,collapse = ' ')})
le=le.expand(h)
is.le=1+apply(perm,2,function(x){any(apply(le,2,function(y){all(x==y)}))})
cols=c('lightgrey','lightblue')[is.le]
ord=apply(X,2,function(x){
which(apply(perm,2,function(y){all(x==y)}))
})
ft=table(ord)/T
barplot(ft~perm.str,las=2,cex.names=0.7,col=cols,
ylab='probability',xlab='permutation',
main=paste('distribution of random orders with noise probability = ',p.val),
cex.main=0.8)
legend('topright',legend=c('LE of h','not LE'),
col=c('lightblue','lightgrey'),pch=c(15,15))
